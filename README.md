# *Improving OOV Word Recognition in End-to-End ASR via Lightweight Domain Adaptation with a TED-LIUM2 N-gram Language Model*

---

## Overview

This project investigates the effectiveness of integrating a lightweight, domain-adapted n-gram language model to improve the recognition of out-of-vocabulary (OOV) words in end-to-end ASR systems.  
The experiments focus on TED-LIUM2 speech data and aim to enhance transcription accuracy in spontaneous speech conditions.

The base ASR model used is [`facebook/wav2vec2-base-100h`](https://huggingface.co/facebook/wav2vec2-base-100h), with decoding support using a 4-gram language model trained using KenLM.

---

## Requirements & Environment

All experiments were conducted on the **HÃ¡brÃ³k High-Performance Computing (HPC) Cluster** at the University of Groningen:

- **GPU**: NVIDIA A100 (40GB VRAM)  
- **CPU**: 20 cores  
- **RAM**: 48GB  
- **Platform**: Linux (SLURM-based job scheduler)

This configuration enabled efficient batch inference, decoding, and large-scale evaluation with no memory bottlenecks.

---

## Directory Overview

The key file structure and main components are summarized below:

TEDLIUM_release2/
â”œâ”€â”€ trainingdataset/                      # Cleaned training text for LM training
â”‚   â”œâ”€â”€ dev_stm_cleaned.txt               # Clean dev STM text
â”‚   â””â”€â”€ train_stm_onlytext.txt            # Clean train STM text
â”‚
â”œâ”€â”€ wer/                                  # Word error rate computation framework
â”‚   â”œâ”€â”€ baseline1/                        # Baseline ASR evaluation
â”‚   â”‚   â”œâ”€â”€ reftext.txt
â”‚   â”‚   â”œâ”€â”€ testtext.txt
â”‚   â”‚   â”œâ”€â”€ insert.py                     # Error type counter
â”‚   â”‚   â””â”€â”€ wer.py                        # WER computation
â”‚   â”‚
â”‚   â”œâ”€â”€ baselinelm/                       # Inference + WER using baseline LM
â”‚   â””â”€â”€ own/                              # Inference + WER using domain-adapted LM
â”‚       â”œâ”€â”€ asrfinal.txt / .lower.txt     # ASR outputs before/after normalization
â”‚       â”œâ”€â”€ reftext.txt                   # Gold reference transcription
â”‚       â”œâ”€â”€ clean2_include477.py          # Removes connect errors before WER calc
â”‚       â”œâ”€â”€ insert.py, wer.py             # Detailed WER and error count tools
â”‚       â”œâ”€â”€ clean.py, form.py, lower.py   # Preprocessing scripts
â”‚       â””â”€â”€ check_space_words.py          # Custom error analysis on space issues
â”‚
â”œâ”€â”€ combine_stm.py                        # Merges STM for training
â”œâ”€â”€ convert_sph_to_wav.sh                 # Shell script to convert SPH to WAV
â”œâ”€â”€ inference_lm.py                       # Inference using LM decoding
â”œâ”€â”€ inference_nlm.py                      # Inference without LM
â”œâ”€â”€ inference_own.py                      # Inference using own TED-LIUM2-trained LM
â”œâ”€â”€ oov.py                                # OOV term extraction and tracking
â”œâ”€â”€ OOV_words.txt                         # List of extracted OOV terms
â”œâ”€â”€ split_by_stm.py                       # Audio segmentation by STM
â”œâ”€â”€ wav2vec2_baseline_results.txt         # WER for baseline (no LM)
â”œâ”€â”€ wav2vec2_with_lm_results.txt          # WER for baseline with LM
â”‚
â”œâ”€â”€ 4grams.py                             # Modify lm_4gram.arpa to add </s> and output lm_4gram_corrected.arpa
â”œâ”€â”€ lm_4gram_corrected_fixed.arpa         # Trained ARPA LM (text)
â””â”€â”€ lm_4gram_corrected.binary             # Compiled KenLM binary (for decoding)


---

## Language Model Details

The 4-gram language model was trained using [KenLM](https://github.com/kpu/kenlm), based on guidance from the Hugging Face tutorial:  
ğŸ‘‰ [Boosting Wav2Vec2 with n-grams in Transformers (Colab)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb)

- **Corpus**: Cleaned transcripts from [TED-LIUM2](https://www.openslr.org/19/)  
- **Format**: All `.sph` audio was manually converted to `.wav`  
- **Test setup**: Long test audios were segmented into **30-second chunks** for evaluation

---

## AI Contribution Statement

This project involved the use of ChatGPT-4o (OpenAI) for:

- Generating and fixong Python scripts
All AI-generated code was manually reviewed and modified as needed to ensure correctness.

---

## ğŸ™ Acknowledgements

This work is part of the MSc Voice Technology program and made possible by the **HÃ¡brÃ³k HPC cluster** at the University of Groningen.  
Special thanks to the open-source tools used:  
[KenLM](https://github.com/kpu/kenlm), [Transformers](https://huggingface.co/docs/transformers/index), and [OpenSLR](https://www.openslr.org/19/)

---


